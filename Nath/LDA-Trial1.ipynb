{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "16028\n",
      "LdaModel(num_terms=10351, num_topics=8, decay=0.5, chunksize=2000)\n",
      "[(0, u'0.069*wipe + 0.056*smell + 0.045*scent + 0.041*clean + 0.035*great + 0.028*love + 0.027*fresh'), (1, u'0.091*wipe + 0.024*like + 0.021*one + 0.019*packag + 0.015*get + 0.015*just + 0.013*dri'), (2, u'0.030*babi + 0.024*review + 0.018*just + 0.017*receiv + 0.016*washcloth + 0.014*product + 0.014*look'), (3, u'0.076*wipe + 0.059*use + 0.053*clean + 0.035*clorox + 0.033*love + 0.026*great + 0.024*easi'), (4, u'0.036*clean + 0.035*use + 0.032*glass + 0.027*wipe + 0.022*get + 0.020*lysol + 0.019*car'), (5, u'0.035*product + 0.035*remov + 0.026*makeup + 0.025*skin + 0.025*use + 0.025*face + 0.019*clean'), (6, u'0.081*wipe + 0.049*use + 0.040*babi + 0.025*brand + 0.023*sensit + 0.021*huggi + 0.019*skin'), (7, u'0.056*product + 0.039*great + 0.037*good + 0.033*work + 0.026*price + 0.018*use + 0.014*wipe')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from string import punctuation\n",
    "\n",
    "table = pd.read_excel('C:/Users/Sneha-laptop/Anaconda2/Dow-Project/Filtered Data.xlsx',\n",
    "                     sheetname = 'Sheet1',\n",
    "                     header = 0,\n",
    "                     index_col = 0,\n",
    "                     parse_cols = \"D:E\")\n",
    "\n",
    "\n",
    "Dataformat=list(table.values.flatten())\n",
    "print 'Data loaded'\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stoplist = get_stop_words('en')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "stoplist +=['wipe']\n",
    "\n",
    "Finalwords=[]\n",
    "count=1\n",
    "\n",
    "for i in Dataformat:\n",
    "    if type(i) is not float:\n",
    "        count=count+1\n",
    "        raw = i.lower()\n",
    "        \n",
    "        #replacing punctuations with spaces\n",
    "        for p in list(punctuation):\n",
    "            raw=raw.replace(p,'')\n",
    "            \n",
    "        #tokenizing words\n",
    "        text=tokenizer.tokenize(raw)\n",
    "        \n",
    "        # remove stop words from tokens\n",
    "        tempword= [p for p in text if p not in stoplist]\n",
    "        \n",
    "        #lemmatization\n",
    "        words = [wordnet_lemmatizer.lemmatize(j) for j in tempword]\n",
    "        \n",
    "        # stemming words\n",
    "        stem_tokens = [p_stemmer.stem(i) for i in words]\n",
    "        \n",
    "        #Complete cleaning\n",
    "        Finalwords.append(stem_tokens)\n",
    "\n",
    "#print Finalwords\n",
    "\n",
    "#creating a term dictionary\n",
    "dictionary = corpora.Dictionary(Finalwords)\n",
    "    \n",
    "#converting the corpus into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in Finalwords]\n",
    "\n",
    "\n",
    "#Creating the object for LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=8, id2word = dictionary, passes=50)\n",
    "\n",
    "\n",
    "print count\n",
    "print ldamodel\n",
    "print (ldamodel.print_topics(num_topics=8, num_words=7))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16028\n",
      "LdaModel(num_terms=10351, num_topics=8, decay=0.5, chunksize=2000)\n",
      "[(0, u'0.033*product + 0.033*babi + 0.025*review + 0.025*great + 0.024*receiv + 0.022*washcloth'), (1, u'0.052*product + 0.039*face + 0.038*remov + 0.037*makeup + 0.037*skin + 0.024*use'), (2, u'0.055*fresh + 0.041*go + 0.030*scent + 0.027*shower + 0.019*feel + 0.016*wont'), (3, u'0.052*clean + 0.044*use + 0.028*glass + 0.025*wipe + 0.025*get + 0.021*great'), (4, u'0.049*use + 0.037*wipe + 0.020*wash + 0.018*cloth + 0.017*babi + 0.017*2'), (5, u'0.086*wipe + 0.035*use + 0.032*babi + 0.021*love + 0.020*brand + 0.019*great'), (6, u'0.077*wipe + 0.052*clean + 0.051*use + 0.034*love + 0.029*clorox + 0.025*great'), (7, u'0.072*wipe + 0.019*one + 0.017*like + 0.015*packag + 0.012*get + 0.011*just')]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "#creating a term dictionary\n",
    "dictionary = corpora.Dictionary(Finalwords)\n",
    "    \n",
    "#converting the corpus into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in Finalwords]\n",
    "\n",
    "\n",
    "#Creating the object for LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=8, id2word = dictionary, passes=100)\n",
    "\n",
    "print count\n",
    "print ldamodel\n",
    "print (ldamodel.print_topics(num_topics=8, num_words=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "16028\n",
      "LdaModel(num_terms=10351, num_topics=6, decay=0.5, chunksize=2000)\n",
      "[(0, u'0.057*wipe + 0.052*use + 0.027*babi + 0.019*nose + 0.018*cloth + 0.017*fresh + 0.016*scent + 0.015*old + 0.013*regular + 0.013*great'), (1, u'0.034*remov + 0.028*babi + 0.023*review + 0.022*soft + 0.020*great + 0.020*skin + 0.020*makeup + 0.020*receiv + 0.019*washcloth + 0.017*product'), (2, u'0.085*wipe + 0.027*use + 0.022*babi + 0.021*brand + 0.018*love + 0.015*diaper + 0.014*huggi + 0.014*great + 0.014*sensit + 0.013*like'), (3, u'0.071*wipe + 0.064*clean + 0.056*use + 0.034*great + 0.032*love + 0.029*clorox + 0.019*easi + 0.018*disinfect + 0.018*product + 0.016*smell'), (4, u'0.027*product + 0.024*use + 0.023*wipe + 0.012*dri + 0.011*like + 0.010*clean + 0.010*face + 0.009*one + 0.008*realli + 0.008*feel'), (5, u'0.071*wipe + 0.025*packag + 0.024*get + 0.016*glass + 0.015*contain + 0.014*like + 0.013*one + 0.013*love + 0.012*first + 0.011*open')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from string import punctuation\n",
    "\n",
    "table = pd.read_excel('C:/Users/Sneha-laptop/Anaconda2/Dow-Project/Filtered Data.xlsx',\n",
    "                     sheetname = 'Sheet1',\n",
    "                     header = 0,\n",
    "                     index_col = 0,\n",
    "                     parse_cols = \"D:E\")\n",
    "\n",
    "\n",
    "Dataformat=list(table.values.flatten())\n",
    "print 'Data loaded'\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stoplist = get_stop_words('en')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "#stoplist +='wipe'\n",
    "\n",
    "Finalwords=[]\n",
    "count=1\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "for i in Dataformat:\n",
    "    if type(i) is not float:\n",
    "        count=count+1\n",
    "        raw = i.lower()\n",
    "        \n",
    "        #replacing punctuations with spaces\n",
    "        for p in list(punctuation):\n",
    "            raw=raw.replace(p,'')\n",
    "            \n",
    "        #tokenizing words\n",
    "        text=tokenizer.tokenize(raw)\n",
    "        \n",
    "        # remove stop words from tokens\n",
    "        tempword= [p for p in text if p not in stoplist]\n",
    "        \n",
    "        #lemmatization\n",
    "        words = [wordnet_lemmatizer.lemmatize(j) for j in tempword]\n",
    "        \n",
    "        # stemming words\n",
    "        stem_tokens = [p_stemmer.stem(i) for i in words]\n",
    "        \n",
    "        #Complete cleaning\n",
    "        Finalwords.append(stem_tokens)\n",
    "\n",
    "#print Finalwords\n",
    "\n",
    "#creating a term dictionary\n",
    "dictionary = corpora.Dictionary(Finalwords)\n",
    "    \n",
    "#converting the corpus into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in Finalwords]\n",
    "\n",
    "\n",
    "#Creating the object for LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=6, id2word = dictionary, passes=50)\n",
    "\n",
    "\n",
    "print count\n",
    "print ldamodel\n",
    "print (ldamodel.print_topics(num_topics=6, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "16028\n",
      "LdaModel(num_terms=10322, num_topics=8, decay=0.5, chunksize=2000)\n",
      "[(0, u'0.052*car + 0.051*glass + 0.029*nose + 0.025*get + 0.024*keep + 0.017*recommend'), (1, u'0.030*review + 0.026*product + 0.024*receiv + 0.024*washcloth + 0.018*free + 0.016*tri'), (2, u'0.023*like + 0.021*one + 0.016*get + 0.015*just + 0.015*packag + 0.012*dri'), (3, u'0.058*cloth + 0.053*use + 0.051*babi + 0.033*wash + 0.021*regular + 0.019*still'), (4, u'0.055*babi + 0.038*brand + 0.035*use + 0.027*skin + 0.026*diaper + 0.026*soft'), (5, u'0.034*product + 0.032*makeup + 0.031*remov + 0.030*face + 0.028*use + 0.021*skin'), (6, u'0.079*clean + 0.078*use + 0.040*love + 0.040*great + 0.027*clorox + 0.024*smell'), (7, u'0.054*great + 0.050*price + 0.039*product + 0.027*amazon + 0.025*buy + 0.024*recommend')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from string import punctuation\n",
    "\n",
    "table = pd.read_excel('C:/Users/Sneha-laptop/Anaconda2/Dow-Project/Filtered Data.xlsx',\n",
    "                     sheetname = 'Sheet1',\n",
    "                     header = 0,\n",
    "                     index_col = 0,\n",
    "                     parse_cols = \"D:E\")\n",
    "\n",
    "\n",
    "Dataformat=list(table.values.flatten())\n",
    "print 'Data loaded'\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stoplist = get_stop_words('en')\n",
    "stoplist +=['wipe']\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "Finalwords=[]\n",
    "count=1\n",
    "\n",
    "\n",
    "\n",
    "for i in Dataformat:\n",
    "      \n",
    "    if type(i) is not float:\n",
    "        count=count+1\n",
    "        raw = i.lower()\n",
    "        \n",
    "        #replacing punctuations with spaces\n",
    "        for p in list(punctuation):\n",
    "            raw=raw.replace(p,'')\n",
    "            \n",
    "        #tokenizing words\n",
    "        text=tokenizer.tokenize(raw)\n",
    "        \n",
    "        # remove stop words from tokens\n",
    "        tempword1= [p for p in text if p not in stoplist]\n",
    "        \n",
    "        \n",
    "        #lemmatization\n",
    "        words = [wordnet_lemmatizer.lemmatize(j) for j in tempword1]\n",
    "            \n",
    "                \n",
    "        # stemming words\n",
    "        stem_tokens = [p_stemmer.stem(i) for i in words]\n",
    "        \n",
    "        # remove stop words from tokens\n",
    "        tempword2= [p for p in stem_tokens if p not in stoplist]\n",
    "        \n",
    "        #Complete cleaning\n",
    "        Finalwords.append(tempword2)\n",
    "\n",
    "\n",
    "\n",
    "#creating a term dictionary\n",
    "dictionary = corpora.Dictionary(Finalwords)\n",
    "    \n",
    "#converting the corpus into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(q) for q in Finalwords]\n",
    "\n",
    "\n",
    "#Creating the object for LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(corpus, num_topics=8, id2word = dictionary, passes=50,alpha='auto')\n",
    "\n",
    "\n",
    "print count\n",
    "print ldamodel\n",
    "print (ldamodel.print_topics(num_topics=8, num_words=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
