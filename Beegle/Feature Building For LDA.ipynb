{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "#nltk.download()  # Download text data sets, including stop words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_and_title(text):\n",
    "    \"\"\"Add the title information to the beginning of the text\"\"\"\n",
    "    try:\n",
    "        x = text['Title']\n",
    "        title = x + '. '\n",
    "    except:\n",
    "        title = 'NO_TITLE'\n",
    "    try:\n",
    "        result = title + ' ' + text['Text']\n",
    "    except:\n",
    "        result = title\n",
    "    return (result)\n",
    "\n",
    "def add_double_title(text):\n",
    "    \"\"\"Add the title information to the beginning of the text (and double it, called 'weighting up')\"\"\"\n",
    "    try:\n",
    "        x = text['Title']\n",
    "        title = x + '. ' + x + '. '\n",
    "    except:\n",
    "        title = 'NO_TITLE'\n",
    "    try:\n",
    "        result = title + ' ' + text['Text']\n",
    "    except:\n",
    "        result = title\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def letters_only(text, field):\n",
    "    \"\"\"Replace all non-alphanumeric characters with a space\"\"\"\n",
    "    try:\n",
    "        x = re.sub(\"[^a-zA-Z0-9]\",    # The pattern to search for\n",
    "                   \" \",               # The pattern to replace it with\n",
    "                   text[field] )      # The text to search\n",
    "    except:\n",
    "        return ('byte_code_error_ignore_this_ record')\n",
    "    return (x.lower())\n",
    "\n",
    "def remove_stop_words(text, field, stopwords_set):\n",
    "    \"\"\"Remove stop words from the review text\"\"\"\n",
    "    words = [w for w in text[field].split() if not w in stopwords_set]\n",
    "    return( \" \".join( words ))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"This is a helper function to translate part of speech (POS) for us in the make_lemmas function.\n",
    "    nltk uses pos_tag to determine the POS of a word that is not compatible with the wordnet_lemmatizer.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def make_lemmas(text, field, stopwords_set):\n",
    "    \"\"\"Toeknizes all words in the review and then tags them with the part of speech (POS) they belong to\n",
    "    as a tuple. Each tuple (word, pos) is then lemmatized before stop words are removed and the list is\n",
    "    joined back into a single item/doc\"\"\"\n",
    "    x = word_tokenize(text[field])\n",
    "    #x = word_tokenize(x)\n",
    "    x = nltk.pos_tag(x)\n",
    "    doc = []\n",
    "    for word, part in x:\n",
    "        doc.append(wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "    words = [w for w in doc if not w.lower() in stopwords_set]\n",
    "    x = ( \" \".join( words ))\n",
    "    return(x)\n",
    "\n",
    "def create_neg_stops():\n",
    "    \"\"\"Combine the original list of stop words the negative suffix. For example: 'his', 'they', and 'me' become\n",
    "    'his_neg', 'they_neg', and 'me_neg'.\"\"\"\n",
    "    orig_stops = stopwords.words(\"english\")\n",
    "    neg_stops = []\n",
    "    for i in orig_stops:\n",
    "        neg_stops.append(i+'_neg')\n",
    "    orig_stops.extend(neg_stops)\n",
    "    return(orig_stops)\n",
    "\n",
    "def remove_negated_stop_words(text, field, neg_stops):\n",
    "    \"\"\"Remove all instances of stop words that have the '_neg' suffix.\"\"\"\n",
    "    # Make the text lowercase\n",
    "    x = text[field]\n",
    "    x = x.lower()\n",
    "    \n",
    "    stopwords_set = set(neg_stops)\n",
    "    \n",
    "    # List comprehension that splits the review into words and removes negative stop words\n",
    "    words = [w for w in text[field].split() if not w in stopwords_set]\n",
    "    return( \" \".join( words ))\n",
    "\n",
    "#/////////////////////ACCOUNT FOR NEGATION SENTIMENT///////////////////////////////////\n",
    "def negatize(text, field):\n",
    "    \"\"\"Use the NLTK library's mark_negation to find negative words (like 'not' and 'nor') and append\n",
    "    the '_neg' suffix to all words following the first negative word until it encounters a period or comma.\n",
    "    Example: 'I don't like eating pizza, I love eating pizza' \n",
    "    becomes 'I don't like_neg eating_neg pizza_neg, I love pizza' \"\"\"\n",
    "    x = text[field]\n",
    "    \n",
    "    # The TextBlob class provides an easy way to split the reviews into sentences\n",
    "    x = TextBlob(x)\n",
    "    \n",
    "    piece = []\n",
    "    for sentence in x.sentences:\n",
    "        \n",
    "        # Split sentence on commas to ID phrases that need to be negated (if required)\n",
    "        part = re.split(', ',str(sentence))\n",
    "        for i in part:\n",
    "            piece.append(mark_negation(i.split()))\n",
    "    \n",
    "    # Combine all terms/phrases back to one doc\n",
    "    total = []\n",
    "    for terms in piece:\n",
    "        total.append(\" \".join(terms))\n",
    "    review = ''\n",
    "    for phrase in total:\n",
    "        review += phrase + ' '\n",
    "    \n",
    "    # mark_negation adds the _NEG suffix after the period, this catches those and fixes it\n",
    "    review = review.replace(\"._NEG\",\"_NEG.\")\n",
    "    review = review.lower()\n",
    "    \n",
    "    # return the entire entire review except for the last character which is always a space\n",
    "    return (review[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data...\n",
      "Finished In:     0.369s.\n",
      "Adding titles to text...\n",
      "Finished In:     0.589s.\n",
      "Removing Non-Alphanumerics...\n",
      "Finished In:     0.777s.\n",
      "Removing Stop Words...\n",
      "Finished In:     0.609s.\n",
      "Tagging Negative Text...\n",
      "Finished In:     9.357s.\n",
      "Removing Negative Stop Words...\n",
      "Finished In:     19.144s.\n",
      "Lemmatizing The Text...\n",
      "Finished In:     549.929s.\n",
      "FINISHED: \n",
      " Time Elapsed:    581.779s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "t1 = time()\n",
    "#/////////////////////READ THE DATA////////////////////////////////////////////////////\n",
    "print ('Reading the data...')\n",
    "wipes = pd.read_csv(\"home_products.csv\", header=0, encoding=\"ISO-8859-1\" )\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////ADD THE TITLE TO THE TEXT////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Adding titles to text...')\n",
    "wipes['text_and_title'] = wipes.apply(lambda text: text_and_title(text), axis=1)\n",
    "wipes['double_title'] = wipes.apply(lambda text: add_double_title(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////REMOVE NON-ALPHANUMERICS AND PUNCTUATION/////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Non-Alphanumerics...')\n",
    "wipes['text_and_title_no_stops'] = wipes.apply(lambda text: letters_only(text, 'text_and_title'), axis=1)\n",
    "wipes['double_title_no_stops'] = wipes.apply(lambda text: letters_only(text, 'double_title'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////LOAD THE STOPWORDS PROVIDED BY NLTK//////////////////////////////\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "#/////////////////////REMOVE STOP WORDS AND PUNCTUATION////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Stop Words...')\n",
    "wipes['text_and_title_no_stops'] = wipes.apply(lambda text: remove_stop_words(text, 'text_and_title_no_stops', stopwords_set), axis=1)\n",
    "wipes['double_title_no_stops'] = wipes.apply(lambda text: remove_stop_words(text, 'double_title_no_stops', stopwords_set), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////NEGATE TEXT AND TITLES///////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Tagging Negative Text...')\n",
    "wipes['text_and_title_negation'] = wipes.apply(lambda text: negatize(text, 'text_and_title'), axis=1)\n",
    "wipes['double_title_negation'] = wipes.apply(lambda text: negatize(text, 'double_title'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////REMOVE NEGATIVE STOPS////////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Negative Stop Words...')\n",
    "wipes['text_and_title_negation_no_stops'] = wipes.apply(lambda text: remove_negated_stop_words(text, 'text_and_title_negation', create_neg_stops()), axis=1)\n",
    "wipes['double_title_negation_no_stops'] = wipes.apply(lambda text: remove_negated_stop_words(text, 'double_title_negation', create_neg_stops()), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////LEMMATIZE THE TEXT REVIEWS///////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Lemmatizing The Text...')\n",
    "wipes['lemma_text_title_no_stops'] = wipes.apply(lambda text: make_lemmas(text, 'text_and_title_no_stops', stopwords_set), axis=1)\n",
    "wipes['lemma_double_title_no_stops'] = wipes.apply(lambda text: make_lemmas(text, 'double_title_no_stops', stopwords_set), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////WRITE THE DATA///////////////////////////////////////////////////\n",
    "wipes.to_csv('home_products_additional_features.csv', index=False)\n",
    "\n",
    "print(\"FINISHED: \\nTime Elapsed:    %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
